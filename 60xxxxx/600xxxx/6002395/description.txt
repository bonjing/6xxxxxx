

"An Introduction to the Modeling of Neural Networks" by Pierre Peretto
Collection Alea-Saclay: Monographs and Texts in Statistical Physics
Cambridge University Press | 1994 dig.print 2004| ISBN: 0521424879, 0521414512 | 492 pages | PDF 

This text is a graduate-level introduction to neural networks, focusing on current theoretical models, examining what these models can reveal about how the brain functions, and discussing the ramifications for psychology, artificial intelligence, and the construction of a new generation of intelligent computers.

The book is divided into four parts. The first part gives an account of the anatomy of the central nervous system, followed by a brief introduction to neurophysiology.
The second part is devoted to the dynamics of neuronal states, and demonstrates how very simple models may stimulate associative memory.
The third part of the book discusses models of learning, including detailed discussions on the limits of memory storage, methods of learning and their associated models, associativity, and error correction.
The final section of the book reviews possible applications of neural networks in artificial intelligence, expert systems, optimization problems, and the construction of actual neuronal supercomputers, with the potential for one-hundred fold increase in speed over contemporary serial machines.


Contents
Preface
Acknowledgments
1 Introduction
1.1 Mind as an emergent property of nervous systems
1.2 Neuronal nets as automata networks: a brief historical overview
1.3 Organization of the book
2 The biology of neural networks: a few features for the sake of non-biologists
2.1 Three approaches to the study of the functioning of central nervous systems
2.2 The anatomy of central nervous systems
2.3 A brief survey of neurophysiology
2.4 Learning and memory: a summary of experimental observations
3 The dynamics of neural networks: a stochastic approach
3.1 Introducing the problem
3.2 Noiseless neural networks
3.3 Taking synaptic noise into account
4 Hebbian models of associative memory
4.1 Noiseless Hebbian models
4.2 Stochastic Hebbian neural networks in the limit of finite numbers of memorized patterns
4.3 Storing an infinite number of patterns in stochastic Hebbian networks: the technique of field distributions
4.4 The replica method approach
4.5 General dynamics of neural networks
5 Temporal sequences of patterns
5.1 Parallel dynamics
52 tochastic dynamics
5.3 An example of conditioned behavior
6 The problem of learning in neural networks
6.1 Introducing the problem
6.2 Linear separability
6.3 Computing the volume of solutions
7 Learning dynamics in 'visible* neural networks
7.1 A classification of learning dynamics
7.2 Constraining the synaptic efficacies
7.3 Projection algorithms
7.4 The perceptron learning rules
7.5 Correlated patterns
8 Solving the problem of credit assignment
8.1 The back-propagation algorithm
8.2 Handling internal representations
8.3 Learning in Boolean networks
9 Self-organization
9.1 Self-organization in simple networks
9.2 Ontogenesis
9.3 Three questions about learning
10 Neurocomputat ion
10.1 Domains of applications of neural networks
10.2 Optimization
10.3 Low-level signal processing
10.4 Pattern matching
10-5 Some speculations on biological systems 10.6 Higher associative functions
11 Neurocomputers
11.1 General principles of neurocomputation
11.2 Semi-paxallel neurocomputers
12 A critical view of the modeling of neural networks
12.1 Information structures the biological system
12.2 The neural code
12.3 The synfire chains
12.4 Computing with attractors versus computing with flows of information
12.5 The issue of low neuronal activities
12.6 Learning and cortical plasticity
12.7 Taking the modular organization of the cortex into account
12.8 Higher-order processing: the problem of artificial intelligence
12.9 Concluding remarks
References
Index